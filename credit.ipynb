{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7247c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f22321c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio HHI: 0.0228\n",
      "Top 3 Borrowers hold 12.9% of total risk.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP: Create a \"Lumpy\" Portfolio (Pareto)\n",
    "# ==========================================\n",
    "np.random.seed(42)\n",
    "n_names = 50  # Small number of large borrowers (Concentrated)\n",
    "\n",
    "# Use Pareto (Power Law) for exposures to simulate \"Lumpiness\"\n",
    "# Shape=3.0 gives a realistic \"Fat Head\" (A few massive loans)\n",
    "exposures = (np.random.pareto(a=3.0, size=n_names) + 1) * 10_000_000\n",
    "exposures = np.sort(exposures)[::-1]  # Sort: Largest first\n",
    "total_exposure = np.sum(exposures)\n",
    "\n",
    "# Check Concentration (HHI Index)\n",
    "hhi = np.sum((exposures / total_exposure)**2)\n",
    "print(f\"Portfolio HHI: {hhi:.4f}\")\n",
    "print(\n",
    "    f\"Top 3 Borrowers hold {np.sum(exposures[:3])/total_exposure*100:.1f}% of total risk.\")\n",
    "\n",
    "# Setup Risk Parameters\n",
    "pds = np.random.uniform(0.001, 0.01, n_names)  # Low PD (High Quality Banks)\n",
    "lgd = 0.45\n",
    "rho = 0.20\n",
    "thresholds = stats.norm.ppf(pds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfc8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed255ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Shift (mu) for this portfolio: -3.0259\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the target (The VaR you just found)\n",
    "target_loss = 28_725_000\n",
    "\n",
    "# 2. Define the objective function\n",
    "# \"Find the Z value where the Smooth Portfolio Loss equals $28.7M\"\n",
    "\n",
    "\n",
    "def z_optimization_func(z_val):\n",
    "    # Calculate Conditional PDs at this Z\n",
    "    cond_pds = stats.norm.cdf(\n",
    "        (thresholds - np.sqrt(rho)*z_val) / np.sqrt(1-rho))\n",
    "    # Calculate Expected Loss (Smooth)\n",
    "    expected_loss = np.sum(cond_pds * exposures * lgd)\n",
    "    return expected_loss - target_loss\n",
    "\n",
    "\n",
    "# 3. Solve for optimal Mu\n",
    "try:\n",
    "    sol = root_scalar(z_optimization_func, bracket=[0, -5], method='brentq')\n",
    "    optimal_mu = sol.root\n",
    "    print(f\"Optimal Shift (mu) for this portfolio: {optimal_mu:.4f}\")\n",
    "except:\n",
    "    print(\"Optimization failed (Target might be unreachable with Smooth model)\")\n",
    "    optimal_mu = -3.5  # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc0cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COMPARISON (99.9% Confidence) ---\n",
      "Smooth VaR (Level 1): $29,423,844\n",
      "Lumpy VaR  (Level 2): $41,060,994\n",
      "Concentration Add-on: $11,637,150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. LEVEL 2 SIMULATION (Exponential Twisting)\n",
    "# ==========================================\n",
    "n_sims = 20000\n",
    "losses_lumpy = []\n",
    "weights_lumpy = []\n",
    "losses_smooth = []\n",
    "weights_smooth = []\n",
    "\n",
    "# IS Parameters\n",
    "shift_mu = -2.3843      # Macro Stress (Level 1)\n",
    "twist_factor = 1\n",
    "\n",
    "# Pre-generate Macro Factors (Shifted)\n",
    "z_shifted = np.random.normal(shift_mu, 1, n_sims)\n",
    "\n",
    "for z in z_shifted:\n",
    "    # A. Macro Weight (Level 1)\n",
    "    w_z = np.exp(-shift_mu * z + 0.5 * shift_mu**2)\n",
    "\n",
    "    # B. Calculate Real Conditional PDs (p)\n",
    "    cond_pd_real = stats.norm.cdf((thresholds - np.sqrt(rho)*z)/np.sqrt(1-rho))\n",
    "\n",
    "    # C. Twist the PDs (q) for simulation\n",
    "    # We force them to default 10x more often than the model says\n",
    "    cond_pd_twisted = np.minimum(cond_pd_real * twist_factor, 0.999)\n",
    "\n",
    "    # D. SIMULATE DISCRETE DEFAULTS (Level 2 - Lumpy)\n",
    "    u_vec = np.random.rand(n_names)\n",
    "    defaults = (u_vec < cond_pd_twisted).astype(int)\n",
    "    loss_lumpy = np.sum(defaults * exposures * lgd)  # Discrete Outcome\n",
    "\n",
    "    # E. Calculate Weight Correction (Likelihood Ratio)\n",
    "    # Vectorized sum of log-weights for all 50 banks\n",
    "    log_w_def = np.log(cond_pd_real) - np.log(cond_pd_twisted)\n",
    "    log_w_surv = np.log(1 - cond_pd_real) - np.log(1 - cond_pd_twisted)\n",
    "\n",
    "    log_w_idiosyncratic = np.sum(\n",
    "        defaults * log_w_def + (1-defaults) * log_w_surv)\n",
    "    w_eps = np.exp(log_w_idiosyncratic)\n",
    "\n",
    "    # --- NEW: CALCULATE SMOOTH LOSS (Level 1) ---\n",
    "    # This assumes \"Infinite Granularity\" (Idiosyncratic risk = 0)\n",
    "    # We just sum the Expected Loss for every bank given the macro state Z\n",
    "    loss_smooth = np.sum(cond_pd_real * exposures * lgd)\n",
    "\n",
    "    # Append both\n",
    "    losses_lumpy.append(loss_lumpy)\n",
    "    losses_smooth.append(loss_smooth)\n",
    "\n",
    "    # Weights are identical for Level 1 analysis (only depends on Z)\n",
    "    weights_lumpy.append(w_z * w_eps)\n",
    "    weights_smooth.append(w_z)  # Level 1 only needs the Z-weight\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. VISUALIZATION & METRICS\n",
    "# ==========================================\n",
    "losses_lumpy = np.array(losses_lumpy)\n",
    "weights_lumpy = np.array(weights_lumpy)\n",
    "\n",
    "# Calculate 99.9% VaR (Weighted)\n",
    "\n",
    "\n",
    "def get_weighted_quantile(data, weights, quantile):\n",
    "    data = np.array(data)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    idx = np.argsort(data)\n",
    "\n",
    "    # Now this will work because 'data' is a numpy array\n",
    "    sorted_data = data[idx]\n",
    "    sorted_w = weights[idx] / np.sum(weights)\n",
    "\n",
    "    cum_w = np.cumsum(sorted_w)\n",
    "    cutoff = np.searchsorted(cum_w, quantile)\n",
    "\n",
    "    # Safety check if cutoff hits the very end\n",
    "    if cutoff >= len(sorted_data):\n",
    "        cutoff = len(sorted_data) - 1\n",
    "\n",
    "    return sorted_data[cutoff]\n",
    "\n",
    "\n",
    "# 1. Calculate the Smooth VaR using the same function\n",
    "var_999_smooth = get_weighted_quantile(losses_smooth, weights_smooth, 0.999)\n",
    "var_999_lumpy = get_weighted_quantile(losses_lumpy, weights_lumpy, 0.999)\n",
    "\n",
    "granularity_addon = var_999_lumpy - var_999_smooth\n",
    "\n",
    "print(f\"--- COMPARISON (99.9% Confidence) ---\")\n",
    "print(f\"Smooth VaR (Level 1): ${var_999_smooth:,.0f}\")\n",
    "print(f\"Lumpy VaR  (Level 2): ${var_999_lumpy:,.0f}\")\n",
    "print(f\"Concentration Add-on: ${granularity_addon:,.0f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e413438",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (271933439.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.savefig{\"comparison.eps\"}\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 2. Plotting: Focus on the Tail\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Define \"Tail Focus\" area: Start plotting from the 99% quantile\n",
    "# This hides the 99% of \"boring\" small losses so we can see the heavy tail detail\n",
    "tail_start = get_weighted_quantile(losses_smooth, weights_smooth, 0.99)\n",
    "tail_end = np.max(losses_lumpy) * 1.05  # Add 5% buffer\n",
    "\n",
    "# Create bins specifically for this tail region\n",
    "bins = np.linspace(tail_start, tail_end, 60)\n",
    "\n",
    "# Plot Smooth (Level 1) - Use 'step' type to make it look like a theoretical curve\n",
    "plt.hist(losses_smooth, weights=weights_smooth, bins=bins, density=True,\n",
    "         histtype='step', linewidth=2, color='orange', label='Smooth (Level 1)')\n",
    "\n",
    "# Plot Lumpy (Level 2) - Use 'bar' type to show the \"jags\"\n",
    "plt.hist(losses_lumpy, weights=weights_lumpy, bins=bins, density=True,\n",
    "         alpha=0.4, color='blue', label='Lumpy (Level 2)')\n",
    "\n",
    "# Add Vertical VaR Lines\n",
    "plt.axvline(var_999_smooth, color='darkorange',\n",
    "            linestyle='--', linewidth=2, label='Smooth VaR')\n",
    "plt.axvline(var_999_lumpy, color='blue', linestyle='--',\n",
    "            linewidth=2, label='Lumpy VaR')\n",
    "\n",
    "# Formatting\n",
    "# <--- THIS IS HOW YOU FOCUS ON THE TAIL\n",
    "plt.xlim(left=tail_start, right=tail_end)\n",
    "plt.xlabel(\"Portfolio Loss ($)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"comparison.eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d767713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COMPARISON (99.9% Confidence) ---\n",
      "Smooth CVaR (Level 1): $37,780,768\n",
      "Lumpy CVaR  (Level 2): $50,983,982\n"
     ]
    }
   ],
   "source": [
    "def get_weighted_cvar(data, weights, var_value):\n",
    "    data = np.array(data)\n",
    "    weights = np.array(weights)\n",
    "    # Filter for tail events only\n",
    "    mask = data >= var_value\n",
    "    tail_losses = data[mask]\n",
    "    tail_weights = weights[mask]\n",
    "    # Weighted Average\n",
    "    return np.sum(tail_losses * tail_weights) / np.sum(tail_weights)\n",
    "\n",
    "\n",
    "cvar_smooth = get_weighted_cvar(losses_smooth, weights_smooth, var_999_smooth)\n",
    "cvar_lumpy = get_weighted_cvar(losses_lumpy, weights_lumpy, var_999_lumpy)\n",
    "print(f\"--- COMPARISON (99.9% Confidence) ---\")\n",
    "print(f\"Smooth CVaR (Level 1): ${cvar_smooth:,.0f}\")\n",
    "print(f\"Lumpy CVaR  (Level 2): ${cvar_lumpy:,.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
